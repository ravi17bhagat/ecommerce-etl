{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7fe5ebc1-0cc2-417a-ae55-d8e9c2b23f67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StringType, DecimalType\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cf76387b-4dac-411b-b068-aa4878a78423",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Data Ingestion to raw table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "11deb001-897b-4e77-ba31-0b7369ae9bfc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def read_csv(spark, file_path):\n",
    "    # Reads a CSV file and returns a DataFrame\n",
    "    df = spark.read.format(\"csv\")\\\n",
    "        .option(\"header\", \"true\")\\\n",
    "        .option(\"inferSchema\", \"true\")\\\n",
    "        .load(file_path)\n",
    "    return df\n",
    "\n",
    "def read_excel(spark, file_path):\n",
    "    # Reads an Excel file and returns a DataFrame\n",
    "    # Requires external library to read excel files\n",
    "    df = spark.read.format(\"com.crealytics.spark.excel\")\\\n",
    "        .option(\"header\", \"true\")\\\n",
    "        .option(\"inferSchema\", \"true\")\\\n",
    "        .load(file_path)\n",
    "    return df\n",
    "\n",
    "def read_json(spark, file_path):\n",
    "    # Reads a JSON file and returns a DataFrame\n",
    "    df = spark.read.format(\"json\")\\\n",
    "        .option(\"inferSchema\", \"true\")\\\n",
    "        .load(file_path)\n",
    "    return df\n",
    "\n",
    "def create_table(df, table_name, path):\n",
    "    # Saves data into Delta tables\n",
    "    df.write.format(\"delta\")\\\n",
    "        .mode(\"append\")\\\n",
    "        .save(f\"/mnt/{path}/{table_name}\")\n",
    "    \n",
    "    # Register table for querying and schema evolution\n",
    "    spark.sql(f\"CREATE TABLE IF NOT EXISTS {table_name} USING DELTA LOCATION '/mnt/{path}/{table_name}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8671c5bc-94d3-402b-b5e7-ab01fe9e5c2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Data Cleanup and creating enriched tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0ca0e5e2-6f2d-4b1b-9bd9-4c6b7b8280dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def format_phone_number(phone):\n",
    "    # Remove non-numeric characters\n",
    "    digits = re.sub(r\"\\D\", \"\", phone)  \n",
    "    if len(digits) == 10:\n",
    "        return f\"({digits[:3]}) {digits[3:6]}-{digits[6:]}\"\n",
    "    return None\n",
    "\n",
    "# Create UDF for above function\n",
    "format_phone_udf = udf(format_phone_number, StringType())\n",
    "\n",
    "def clean_customer_data(df):\n",
    "    # Data cleanup in customer table\n",
    "    # 1. Remove all non-alphabetical characters from Customer Name field\n",
    "    # 2. Standardize Phone field to standard format of (XXX) XXX-XXXX \n",
    "\n",
    "    df_cleaned = df.withColumn(\"Customer Name\", trim(regexp_replace(col(\"Customer Name\"), r\"[^a-zA-Z\\s']\", \"\")))\\\n",
    "                    .withColumn(\"Phone\", format_phone_udf(col(\"phone\")))\n",
    "    return df_cleaned\n",
    "\n",
    "def clean_product_data(df):\n",
    "    # Data cleanup in product table\n",
    "    # 1. Round up price values to 2 decimal places\n",
    "\n",
    "    df_cleaned = df.withColumn(\"price\", round(col(\"price\"), 2).cast(DecimalType(10, 2)))\n",
    "    return df_cleaned\n",
    "\n",
    "def enrich_order_data(order_df, customer_df, product_df):\n",
    "    # Enrich order data to create a master table\n",
    "    # 1. Join customer_df to get Customer Name and Country\n",
    "    # 2. Join product_df to get Category and Sub-Category\n",
    "    # 3. Round up profit values to 2 decimal places\n",
    "\n",
    "    cust_df = customer_df.select(col(\"Customer ID\"), col(\"Customer Name\"), col(\"Country\")).drop_duplicates()\n",
    "    prd_df = product_df.select(col(\"Product ID\"), col(\"Category\"), col(\"Sub-Category\")).drop_duplicates()\n",
    "    order_enrich_df = order_df.join(cust_df, cust_df[\"Customer ID\"]==order_df[\"Customer ID\"], how=\"left\")\\\n",
    "                            .join(prd_df, prd_df[\"Product ID\"]==order_df[\"Product ID\"], how=\"left\")\\              \n",
    "                            .withColumn(\"Profit\", round(col(\"profit\"), 2).cast(DecimalType(10, 2)))\\\n",
    "                            .select([\"Order ID, Order Date\", \"Ship Date\", \"Ship Mode\", \"Customer Name\", \"Country\", \"Category\", \"Sub-Category\", \"Quantity\", \"Price\", \"Discount\", \"Profit\"])\n",
    "    return order_enrich_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b0e92124-5bc7-4058-9d3a-b0767bb06089",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Create Aggregate table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "40f9b678-2895-4f08-a261-b53f426bce68",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def aggregate_profit_data(order_df):\n",
    "    # Aggregates profit by year, category, sub-category, and customer.\n",
    "\n",
    "    # Extract year from the order_date\n",
    "    order_df = order_df.withColumn(\"Year\", year(col(\"Order Date\")))\n",
    "\n",
    "    # Group by year, Category, Sub-category and Customer Name\n",
    "    master_data = order_df.groupBy(\"Year\", \"Category\", \"Sub-Category\", \"Customer Name\") \\\n",
    "                        .agg(sum(\"Profit\").alias(\"profit\"))\n",
    "    master_data = master_data.select([\"Year\", \"Category\", \"Sub-Category\", \"Customer Name\", \"Profit\"])\\\n",
    "            .drop_duplicates()\n",
    "    return master_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b596a16d-1f33-4764-98ba-ab4d819f43ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Main method\n",
    "if __name__ == '__main__':\n",
    "    # Initialize spark session\n",
    "    spark = SparkSession.builder.appName(\"Ecommerce\").getOrCreate()\n",
    "\n",
    "    # File paths (Databricks DBFS or cloud storage)\n",
    "    csv_path = \"/mnt/data/product.csv\"\n",
    "    excel_path = \"/mnt/data/customer.xlsx\"\n",
    "    json_path = \"/mnt/data/order.json\"\n",
    "\n",
    "    # Read files\n",
    "    product_df = read_csv(spark, csv_path)\n",
    "    customer_df = read_excel(spark, excel_path)\n",
    "    order_df = read_json(spark, json_path)\n",
    "\n",
    "    # Create raw tables (Bronze layer)\n",
    "    create_table(product_df, \"raw_product\", 'bronze')\n",
    "    create_table(customer_df, \"raw_customer\", 'bronze')\n",
    "    create_table(order_df, \"raw_order\", 'bronze')\n",
    "\n",
    "    # Data cleanup for customer and product tables (Silver layer)\n",
    "    customer_enriched_df = clean_customer_data(customer_df)\n",
    "    product_enriched_df = clean_product_data(product_df)\n",
    "    create_table(customer_enriched_df, \"enriched_customer\", 'silver')\n",
    "    create_table(product_enriched_df, \"enriched_order\", 'silver')\n",
    "\n",
    "    # Enrich order data (Silver layer)\n",
    "    order_enriched_df = enrich_order_data(order_df, customer_enriched_df, product_enriched_df)\n",
    "    create_table(order_enriched_df, \"enriched_order\", 'silver')\n",
    "\n",
    "    # Create master table for aggregation (Gold layer)\n",
    "    order_master_df = aggregate_profit_data(order_enriched_df)\n",
    "    create_table(order_master_df, \"master_order\", 'gold')\n",
    "\n",
    "    # SQL queries on aggregate data\n",
    "    # Create a temporary view of the aggregated data\n",
    "    order_master_df.createOrReplaceTempView(\"order_master_view\")\n",
    "\n",
    "    # a. Profit by year\n",
    "    profit_by_year_df = spark.sql(\"\"\"\n",
    "        SELECT year, SUM(profit) AS total_profit\n",
    "        FROM order_master_view\n",
    "        GROUP BY year\n",
    "        ORDER BY year\n",
    "    \"\"\").show()\n",
    "\n",
    "    # b. Profit by year + category\n",
    "    profit_by_year_category_df = spark.sql(\"\"\"\n",
    "        SELECT year, Category, SUM(profit) AS total_profit\n",
    "        FROM order_master_view\n",
    "        GROUP BY year, Category\n",
    "        ORDER BY year, Category\n",
    "    \"\"\").show()\n",
    "\n",
    "    # c. Profit by customer\n",
    "    profit_by_customer_df = spark.sql(\"\"\"\n",
    "        SELECT `Customer Name`, SUM(profit) AS total_profit\n",
    "        FROM order_master_view\n",
    "        GROUP BY `Customer Name`\n",
    "        ORDER BY `Customer Name`\n",
    "    \"\"\").show()\n",
    "\n",
    "    # d. Profit by customer + year\n",
    "    profit_by_year_category_df = spark.sql(\"\"\"\n",
    "        SELECT `Customer Name`, year, SUM(profit) AS total_profit\n",
    "        FROM order_master_view\n",
    "        GROUP BY `Customer Name`, year\n",
    "        ORDER BY `Customer Name`, year\n",
    "    \"\"\").show()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Ecommerce_ETL",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
