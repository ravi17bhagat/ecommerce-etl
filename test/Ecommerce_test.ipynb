{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d76e5e71-cea4-4aed-b87b-a0d0b2a2a0bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pytest\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "31284353-f624-45b3-abe9-6828f240d6de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import all functions from Main Notebook\n",
    "%run \"/Workspace/Ecommerce/Ecommerce_ETL\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8cf7577b-5e1b-4deb-8790-c7c7ff9f0d3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create spark session for testing\n",
    "@pytest.fixture\n",
    "def spark():\n",
    "    return SparkSession.builder.appName(\"test\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c071db18-e0b9-4edf-9f64-3fca33617185",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Test data ingestion functions\n",
    "def test_read_csv(spark):\n",
    "    df = read_csv(spark, \"/mnt/test/sample_product.csv\")\n",
    "    assert df is not None\n",
    "    assert df.count() > 0\n",
    "\n",
    "def test_read_excel(spark):\n",
    "    df = read_excel(spark, \"/mnt/test/sample_customer.xlsx\")\n",
    "    assert df is not None\n",
    "    assert df.count() > 0\n",
    "\n",
    "def test_read_json(spark):\n",
    "    df = read_json(spark, \"/mnt/test/sample_order.json\")\n",
    "    assert df is not None\n",
    "    assert df.count() > 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "67aa1ef0-2525-4554-ae7a-3e52f7690d1e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Test data transformation functions\n",
    "def test_clean_customer_data(spark):\n",
    "    test_data = [\n",
    "        (\"John123 Doe!@\", \"123-456-7890\"),\n",
    "        (\"Mary_45 Jane$\", \"(987)654-3210\"),\n",
    "        (\"$#@Mike O'Conner \", \"9876543210\"),\n",
    "    ]\n",
    "    df = spark.createDataFrame(test_data, [\"customer_name\", \"phone_number\"])\n",
    "    transformed_df = clean_customer_data(df)\n",
    "\n",
    "    expected_data = [\n",
    "        (\"John Doe\", \"(123) 456-7890\"),\n",
    "        (\"Mary Jane\", \"(987) 654-3210\"),\n",
    "        (\"Mike O'Conner\", \"(987) 654-3210\"),\n",
    "    ]\n",
    "    expected_df = spark.createDataFrame(expected_data, [\"customer_name\", \"phone_number\"])\n",
    "\n",
    "    assert transformed_df.collect() == expected_df.collect()\n",
    "\n",
    "\n",
    "def test_clean_product_data(spark):\n",
    "    schema = StructType([\n",
    "        StructField(\"product_name\", StringType(), True),\n",
    "        StructField(\"price\", DecimalType(10, 4), True)  # Original precision\n",
    "    ])\n",
    "    \n",
    "    test_data = [(\"Product A\", 10.4567), (\"Product B\", 20.7899), (\"Product C\", 30.1)]\n",
    "    df = spark.createDataFrame(test_data, schema=schema)\n",
    "    \n",
    "    transformed_df = clean_product_data(df)\n",
    "\n",
    "    expected_data = [(\"Product A\", 10.46), (\"Product B\", 20.79), (\"Product C\", 30.10)]\n",
    "    expected_schema = StructType([\n",
    "        StructField(\"product_name\", StringType(), True),\n",
    "        StructField(\"price\", DecimalType(10, 2), True)  # Expected precision\n",
    "    ])\n",
    "    expected_df = spark.createDataFrame(expected_data, schema=expected_schema)\n",
    "\n",
    "    assert transformed_df.collect() == expected_df.collect()\n",
    "\n",
    "def test_enrich_order_data(spark):\n",
    "    # Sample input data for order_df\n",
    "    order_data = [\n",
    "        (1, \"2025-01-01\", \"2025-01-02\", \"Standard\", 101, 201, 10, 100, 0.1, 900),\n",
    "        (2, \"2025-01-02\", \"2025-01-03\", \"Express\", 102, 202, 5, 200, 0.05, 950)\n",
    "    ]\n",
    "    order_columns = [\"Order ID\", \"Order Date\", \"Ship Date\", \"Ship Mode\", \"Customer ID\", \"Product ID\", \"Quantity\", \"Price\", \"Discount\", \"Profit\"]\n",
    "    order_df = spark.createDataFrame(order_data, order_columns)\n",
    "\n",
    "    # Sample input data for customer_df\n",
    "    customer_data = [\n",
    "        (101, \"John Doe\", \"USA\"),\n",
    "        (102, \"Jane Smith\", \"UK\")\n",
    "    ]\n",
    "    customer_columns = [\"Customer ID\", \"Customer Name\", \"Country\"]\n",
    "    customer_df = spark.createDataFrame(customer_data, customer_columns)\n",
    "\n",
    "    # Sample input data for product_df\n",
    "    product_data = [\n",
    "        (201, \"Electronics\", \"Mobile\"),\n",
    "        (202, \"Furniture\", \"Table\")\n",
    "    ]\n",
    "    product_columns = [\"Product ID\", \"Category\", \"Sub-Category\"]\n",
    "    product_df = spark.createDataFrame(product_data, product_columns)\n",
    "\n",
    "    # Call the function to enrich the order data\n",
    "    enriched_df = enrich_order_data(order_df, customer_df, product_df)\n",
    "\n",
    "    # Expected output data\n",
    "    expected_data = [\n",
    "        (1, \"2025-01-01\", \"2025-01-02\", \"Standard\", \"John Doe\", \"USA\", \"Electronics\", \"Mobile\", 10, 100, 0.1, Decimal('900.00')),\n",
    "        (2, \"2025-01-02\", \"2025-01-03\", \"Express\", \"Jane Smith\", \"UK\", \"Furniture\", \"Table\", 5, 200, 0.05, Decimal('950.00'))\n",
    "    ]\n",
    "    expected_columns = [\"Order ID\", \"Order Date\", \"Ship Date\", \"Ship Mode\", \"Customer Name\", \"Country\", \"Category\", \"Sub-Category\", \"Quantity\", \"Price\", \"Discount\", \"Profit\"]\n",
    "    \n",
    "    expected_df = spark.createDataFrame(expected_data, expected_columns)\n",
    "\n",
    "    # Compare the actual result with the expected result using exceptAll\n",
    "    diff_df = enriched_df.exceptAll(expected_df)\n",
    "\n",
    "    # Assert that there are no differences between the two DataFrames\n",
    "    assert diff_df.count() == 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f5dd6b4f-5bd7-4c58-be3c-9e35919f55c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Test data aggregation function\n",
    "\n",
    "def test_aggregate_profit_data(spark):\n",
    "    # Sample input data for order_df\n",
    "    order_data = [\n",
    "        (\"2025-01-01\", \"Electronics\", \"Mobile\", \"John Doe\", 100),\n",
    "        (\"2025-02-01\", \"Electronics\", \"Mobile\", \"John Doe\", 200),\n",
    "        (\"2025-03-01\", \"Furniture\", \"Chair\", \"Jane Smith\", 150),\n",
    "        (\"2025-03-01\", \"Furniture\", \"Chair\", \"John Doe\", 100),\n",
    "        (\"2026-01-01\", \"Electronics\", \"Laptop\", \"John Doe\", 300)\n",
    "    ]\n",
    "    order_columns = [\"Order Date\", \"Category\", \"Sub-Category\", \"Customer Name\", \"Profit\"]\n",
    "    order_df = spark.createDataFrame(order_data, order_columns)\n",
    "\n",
    "    # Call the function to aggregate profit data\n",
    "    aggregated_df = aggregate_profit_data(order_df)\n",
    "\n",
    "    # Expected output data (after aggregation)\n",
    "    expected_data = [\n",
    "        (2025, \"Electronics\", \"Mobile\", \"John Doe\", Decimal('300.00')),\n",
    "        (2025, \"Furniture\", \"Chair\", \"Jane Smith\", Decimal('150.00')),\n",
    "        (2025, \"Furniture\", \"Chair\", \"John Doe\", Decimal('100.00')),\n",
    "        (2026, \"Electronics\", \"Laptop\", \"John Doe\", Decimal('300.00'))\n",
    "    ]\n",
    "    expected_columns = [\"Year\", \"Category\", \"Sub-Category\", \"Customer Name\", \"Profit\"]\n",
    "\n",
    "    # Create the expected DataFrame\n",
    "    expected_df = spark.createDataFrame(expected_data, expected_columns)\n",
    "\n",
    "    # Compare the actual result with the expected result using exceptAll\n",
    "    diff_df = aggregated_df.exceptAll(expected_df)\n",
    "\n",
    "    # Assert that there are no differences between the two DataFrames\n",
    "    assert diff_df.count() == 0\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Ecommerce_test",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
